\documentclass[a4paper, 12pt, titlepage]{report}

\usepackage[english]{babel}
\usepackage{listings}
\usepackage{setspace}
\usepackage{color}
\usepackage{mathptmx}
\usepackage{titlesec}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{amsmath}

\newcommand{\emptypage}{\newpage\shipout\null}
\newcommand{\autori}[1]{\textsc{\texttt{#1}}}
\lstset
{
	basicstyle=\scriptsize\ttfamily,keywordstyle=\color{OliveGreen},
	commentstyle=\color{blue},stringstyle=\color{red},showstringspaces=false,
   	breaklines=true,tabsize=4, frame=single, rulecolor=\color{black}
}

\newenvironment{dedication}
  {\clearpage           % we want a new page
   \thispagestyle{empty}% no header and footer
   \vspace*{\stretch{1}}% some space at the top 
   \itshape             % the text is in italics
   \raggedleft          % flush to the right margin
  }
  {\par % end the paragraph
   \vspace{\stretch{3}} % space at bottom is three times that at the top
   \clearpage           % finish off the page
  }
 
\renewcommand\labelitemii{--}

\begin{document} \sloppy

\begin{titlepage}
High performance networking extensions for VirtualBox
\end{titlepage}

\emptypage

\begin{dedication}
Alla mia famiglia,\\
a Erika,\\
ai miei amici.
\end{dedication}

\begin{abstract}
Virtual Machine systems are commonly used in several organizations providing network services, since those systems supply high reliability, security and availability. Therefore, network performance has become a critical issue to deal with, since Virtual Machine systems are widespread nowadays.
\\
\\
In this thesis we are going to present VirtualBox~\cite{vbox} hypervisor, giving some details about its architecture and analyzing network performances of the existing solution. We then implement an extension that interfaces the hypervisor with netmap framework~\cite{netmap}, which provides fast packet I/O. Finally, we present some optimizations to an emulated network device (e1000 in our case), that considerably improve network performances.
\end{abstract}
\emptypage
\tableofcontents

\chapter{Introduction}

It is important to point out that the term \textit{Virtual Machine} may have multiple interpretations, so we must first specify which one of those meanings we are referring to.

When we talk about Virtual Machine (VM), we refer to a \textit{virtualized} computing environment running on top of a physical computing environment; as a result, we get one or more independent VMs, which may be different from the original one.\newline
Before proceeding, we introduce some terminology:
\begin{itemize}
\item \textit{Guest}: The VM.
\item \textit{Host}: The physical computing environment that \textit{hosts} one or more VMs.
\item \textit{Virtual Machine Monitor (VMM)}: The software part that provides support for virtualization. Also known as \textit{Hypervisor}.
\end{itemize}

The main reason that caused the spread of VMs is the abstraction levels that it introduces; this brings many benefits:
\begin{itemize}
\item \textit{Flexibility}: you can run programs compiled for a given Instruction Set Architecture (ISA) and/or a given Operating System (OS) on top of a computer that has a different ISA and/or different OS (e.g. you can test new software on different architectures without \textbf{having} one machine per architecture).
\item \textit{Protection}: each guest is \textbf{isolated}, which means that you can execute different applications on different VMs, so that if an application has a security issue, only the VM (or VMs) running that specific software will be exposed to it.
\item \textit{Resources usage}: one single physical machine may provide multiple services using the 100\% of the resources, instead of using many underutilized physical machines, thus reducing costs and saving energy.
\item \textit{Mobility}: replicating VMs to other locations is only a matter of copying/transmitting some files; this helps avoiding multiple setups since through a VM you can bring a functioning computing environment ready to use to the user.
\end{itemize}

As stated before, the term \textit{Virtual Machine} may have several meanings. A generic architecture of the "class" of VMs we will refer to (called \textit{Type 2 System Virtual Machines}), is shown in figure 1.1. In this case the VMM is a regular OS process, that runs in the host OS along with other processes. The VMM can access the physical resources through the OS services, which depend on the specific OS. We will not investigate other classes of Virtual Machines since this topic falls outside this work. 
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.5]{img/vm_arch.png}
	\caption{Type 2 System Virtual Machine generic architecture}
\end{figure}

\section{Virtual Machine Implementation}
The basic idea behind VMs, is to \textit{emulate}, i.e. to execute code written for a certain environment, using another environment. In the following, we will briefly present the three basic techniques to implement emulation: interpretation, dynamic translation, hardware-based virtualization.
\subsection{Interpretation}
This is the naive emulation technique. The VMM has to perform in software what a physical CPU would have done in hardware: so it will be implemented as a loop, for each iteration, performs  the fetch, decode and execute phases of instruction execution.
\\
\\
Writing an interpreter for a modern ISA can be a very long and difficult process, even if it is conceptually simple; in fact, it is just a matter of reading an Instruction Set specification and implement all the possible instructions respecting the specifications.\\
However, the simplicity of this approach is responsible for its inefficiency; as a matter of fact, for each source instruction, the VMM has to execute many host instructions (e.g. 30-100) to perform in software all the necessary operations. The average \textit{translation ratio} is very high (e.g. 40).
\subsection{Dynamic translation} \label{subsec:dyn_tr}
This is a more sophisticated form of emulation. Rather than performing a "source-code-to-source-code" translation, the idea is to translate it into an equivalent binary code that can be executed directly on the host CPU.\\
This method amortizes the cost of interpretation, doing the fetch and decode phases only once or a few times. The code execution step of an instruction or a block of instructions is generated once (or a few times) and stored in a \textit{Code Cache}. After some time the code cache will contain the complete translation of the source program into the host ISA.\newline
As a result, the average translation ratio can be close to 1, giving an acceptable performance.
\\
\\
This technique is way more complicated than the previous one. In this case, several problems are present:
\begin{itemize}
\item \textit{code-discovery}: makes impossible to do static translation
\item \textit{code-location}: different address space of the guest and host systems
\item \textit{state mapping}: the way the VMM maps guest registers and the like to the host ones
\end{itemize}

It is interesting to notice that both interpretation and dynamic translation can make sense also in the case that guest and host have the same ISA; if this is the case, the translation is simplified since the code can be natively executed on the host machine, without performance losses.\\
However there are some cases where emulation in software my be necessary. As a typical example, memory accesses to the I/O space may need software emulation. In particular, if the guest wants to access a physical resource that is present on the host (e.g. a network adapter), the VMM cannot allow direct access to the device, because other processes could be accessing the same device at the same time, and, obviously, the host network driver and the guest network driver are not aware of each other. On the other hand, if the guest wants to access a virtual device (which does not exist on the host), the I/O instruction must be \textit{trapped}\footnote{Guest execution is interrupted and the VMM takes control.} in order to emulate the device behavior in software.

\subsection{Hardware-based virtualization} \label{subsec:hw_virt}
Due to the widespread use of VMs, extensions for virtualization were introduced by processor vendors. Thanks to these hardware assists, some of the problems affecting dynamic translation techniques have been overcome, and at the same time they have made it easier to execute guest code natively. Both AMD and Intel proposed their extensions for the x86 ISA, AMD-v (\cite{amd-v}) and VT-x (\cite{vt-x}) respectively.
\\
\\
With this new extension, the CPU can execute in two different modes: \textit{root mode} and \textit{VM mode} (or \textit{non-root mode}). The CPU can switch from root mode to VM mode through a so called \textit{VM entry} instruction, while can switch back to root mode through a so called \textit{VM exit} instruction. When in VM mode, the CPU can execute guest code in a controlled environment, i.e. the CPU cannot execute some safety-critical instructions (e.g. I/O instructions); when necessary, CPU performs a VM exit and runs host code (VMM or other processes). The switch operation between host world and guest world is similar to a context switch, since it involves the saving of the host state and loading the guest state (and vice versa). Although performed in hardware, these transitions between host and guest worlds are expensive in terms of performance, because software overhead, OS operations and userspace/kernelspace transitions are involved in the switching operations, but they are also necessary when dealing with I/O operations or interrupts. Hence, VM switches must be minimized in order to achieve good I/O performances.
\section{I/O Virtualization techniques}
Emulating a device means doing in software what the device would do in hardware. Thus, when a guest accesses an I/O device (e.g. writes to a device register), the VMM must take over and emulate all the operations associated with the specific I/O access.
\\
\\
In order to improve I/O virtualization techniques, three approaches have been defined:
\begin{itemize}
\item Hardware support in the devices (\textit{virtual functions} and IOMMU \cite{iommu}), so that a guest can directly access devices in a protected way and run at native speed.
\item Runtime optimizations in VMM. E.g. running short code involving multiple I/O instructions in interpreted mode saves some VM exits\footnote{See \cite{vmm_opt} for details.}.
\item Design \textit{virtual} device models in order to reduce expensive operations in device emulations (e.g. I/O accesses and interrupts). This approach is known as \textit{device paravirtualization} and produced some virtual device models, such as VirtIO (\cite{virtio}).
This requires synchronization and memory sharing between the guest and VMM in order to exchange information, while interrupts are used only for notification purposes. In that way it is easier to minimize the amount of VM exits.
\end{itemize}
\chapter{VirtualBox}
In this chapter we will present VirtualBox hypervisor, giving details about its features (section~\ref{sec:features}), its internal architecture (section~\ref{sec:architecture}), and how the behavior of e1000 device is emulated (section~\ref{sec:e1000_emu}).
\\
As host OS, we used Ubuntu 15.10 64-bit with kernel version 4.2.0. The guest OS is generally the same as the host OS host, but some tests have been performed with FreeBSD as guest too.

\section{VirtualBox features} \label{sec:features}
VirtualBox is a free, open source hypervisor, written entirely in C/C++. In particular, it is a cross-platform type 2 VMM, so it is able to run an arbitrary OS, regardless of the host OS, and it is implemented as a regular process in the host OS, therefore it can make use of all OS services. At the time of the writing, VirtualBox version number is 5.0.4, so we will refer to that version for Linux OS (since our host OS is Linux based).
\\
\\
Here is a brief outline of VirtualBox main features:
\begin{itemize}
\item \textbf{Portability.} VirtualBox runs on a large number of a 32-bit and 64-bit host operating systems. It can run VMs created on different hosts and/or with different virtualization software.
\item \textbf{Multiple virtualization interfaces.} VirtualBox provides three different virtualization interfaces
	\begin{itemize}
	\item \textit{Minimal}: Announces the presence of a virtualized environment.
	\item \textit{KVM}: Presents a Linux KVM hypervisor interface which is recognized by Linux kernels starting with version 2.6.25.
	\item \textit{Hyper-V}: Presents a Microsoft Hyper-V hypervisor interface which is recognized by Windows 7 and newer operating systems.
	\end{itemize}
We chose the KVM interface for tests and implementations.
\item \textbf{Multiple frontends}: A \textit{frontend} is a user interface that VirtualBox provides, such as:
	\begin{itemize}
	\item \textit{VBoxManage}: A textual interface that allows advanced settings for VMs.
	\item \textit{VirtualBox}: The default frontend, based on Qt~\cite{qt}.
	\item \textit{VBoxSDL}: An alternative frontend based on SDL~\cite{sdl}. This is useful for business use as well as testing  during development. The VMs then have to be controlled with VBoxManage.
	\item \textit{VBoxFB}: The "Framebuffer GUI", a GUI that sits directly on the Linux framebuffer. Not currently maintained.
	\end{itemize}
\item \textbf{No hardware virtualization required.} Even if hardware virtualization is fully supported, VirtualBox does not require the processor features such as Intel VT-x or AMD-V; in that way VirtualBox can be used also on old hardware which has not these features.
\item \textbf{Guest Additions.} VirtualBox Guest Additions are software package which can be installed \textit{inside} of supported guest systems to provide additional integration and communication with the host system (e.g. accelerated 3D graphics, automatic adjustment of video resolution and more).
\item \textbf{Great hardware support.} Among others, VirtualBox supports:
	\begin{itemize}
	\item \textbf{Guest multiprocessing (SMP).} VirtualBox can present up to 32 virtual CPUs to each virtual machine, regardless of how many CPU cores are physically present on the host.
	\item \textbf{USB device support}. VirtualBox implements a virtual USB controller that allows to connect arbitrary USB devices to VMs without having to install device-specific drivers on the host.
	\item \textbf{Hardware compatibility.} VirtualBox virtualizes a vast array of virtual devices. That includes IDE, SCSI and SATA hard disk controllers, several virtual network cards (including e1000) and so on.
	\item \textbf{Full ACPI support.} The Advanced Configuration and Power Interface (ACPI) is fully supported by VirtualBox.
	\end{itemize}
\item \textbf{Multigeneration branched snapshots.} VirtualBox can save arbitrary snapshots of the state of the VM. You can go back in time and revert the VM to any such snapshot and start an alternative VM configuration from there, effectively creating a whole snapshot tree.
\item \textbf{VM groups.} VirtualBox provides a groups feature that enables the user to organize and control VMs collectively, as well as individually.
\item \textbf{Clean and modular architecture.} VirtualBox has an extremely modular design with well-defined internal programming interfaces and clean separation of client and server code (i.e. code related to VMs and code related to the VMM, respectively).
\end{itemize}
\section{VirtualBox architecture} \label{sec:architecture}
In this section we will present the internal architecture of VirtualBox, giving details about its implementation that is necessary to understand in order to implement our optimizations.
\subsection{VirtualBox kernel modules}
VirtualBox provides different kernel modules that the user should add to the host kernel:
\begin{itemize}
\item \textit{vboxdrv}: The only mandatory module. This is needed by the VMM to gain control over the host system. It is used to manage the host/guest world switches and device emulations.
\item \textit{vboxnetadp}: "vboxnetadp" stands for "VirtualBox Network Adapter". It is needed to create a host networking interface (called \textit{vboxnet}); that interface (basically a virtual switch), is used to connect VMs to each other (and/or to the host). It is necessary when VirtualBox networking mode is set on \textbf{host-only} or \textbf{bridged} (these modes will be explained in section~\ref{subsec:net_modes}).
\item \textit{vboxnetflt}: "vboxnetflt" stands for "VirtualBox Network Filter". It is a kernel module that attaches to a real interface on the host and filters and injects packets. As for vboxnetadp, it is only necessary for host-only and bridged modes.
\item \textit{vboxpci}: This kernel module provides PCI card passthrough. This is used when the user wants to use a PCI device on the guest, even if the related driver is not available on the host.
\end{itemize}
\subsection{Software Virtualization}
As stated in section~\ref{sec:features}, VirtualBox fully supports hardware virtualization. However, since this is not a requirement, in the event that hardware virtualization is not present, VirtualBox makes use of a technique defined as \textit{Software virtualization}.
In order to understand the software virtualization technique, it is important to understand how CPUs provide a mechanism of protection at microcode level called \textit{Protection rings}.
\subsubsection{Privilege rings}
\begin{figure}[!h]
	\centering
	\includegraphics[scale=0.5]{img/rings.png}
	\caption{Protection rings}
\end{figure}
As shown in figure 2.1, there are four privilege levels or \textit{rings}, numbered from 0 to 3, with ring 0 (R0) being the most privileged and ring 3 (R3) being the least. The use of ring allows for system software to restrict task from accessing data or executing privileged instructions. In most environments, the OS and some device drivers run in R0 and applications run in ring 3~\cite{rings}.
\subsubsection{Software Virtualization}
In addition to the four privilege rings provided bu the hardware, we need to differentiate between \textit{host context} and \textit{guest context}.
\begin{itemize}
\item In \textit{host context}, everything is as if no VMM was active. This might be the active mode if another application on the host has been scheduled CPU time; in that case, there is a host R3 mode and a host R0 mode.
\item In \textit{guest context} a VM is active. So long as the guest code is running in ring 3, this is not much of a problem since a hypervisor can set up the page tables properly and run that code natively on the processor. The problems mostly lie in how to intercept what the guest's kernel does.
\end{itemize}
When starting a VM, through its R0 support kernel driver, VirtualBox has set up the host system so that it can run most of the guest code natively, but it inserted itself at the "bottom" of the picture. It can then assume control when needed, e.g. if a privileged instruction is executed, the guest \textit{traps}; VirtualBox may then handle this and either route a request to a virtual device or possibly delegate handling such things to the guest or host OS. In guest context, VirtualBox can therefore be in one of three states:
\begin{itemize}
\item Guest R3 code is run unmodified, at full speed, as much as possible. The number of faults will generally be low. This is also referred to as \textit{raw mode}, as the guest R3 code runs unmodified.
\item For guest code in R0, VirtualBox employs a trick: it actually reconfigures the guest so that its R0 code is run in \textbf{ring-1} (R1) instead. As a result, when guest R0 code (actually running in R1) such as a guest device driver attempts to write to an I/O register or execute a privileged instruction, the VirtualBox hypervisor in "real" R0 can take over.
\item The VMM can be active. Every time a fault occurs, VirtualBox looks at the offending instruction and can relegate it to a virtual device, the host OS, the guest OS, or run it in the \textbf{recompiler}.
\\
\\
In particular, the recompiler is used when guest code disables interrupts and VirtualBox cannot figure out when they will be switched back on. The recompiler is based on the dynamic translation technique (section~\ref{subsec:dyn_tr}).
\end{itemize}
Unfortunately this only wokrs to a degree. Among others, the following situations require special handling:
\begin{enumerate}
\item Running R0 code in R1 causes a lot of additional instruction faults, as R1 is not allowed to execute any privileged instructions (of which guest's R0 contains plenty). With each of these faults, the VMM must step in and emulate the code to achieve the desired behavior. While this works, emulating thousands of these faults is very expensive and severely hurts the performance of the virtualized guest.
\item There are certain flaws in the implementation of R1 in the x86 architecture that were never fixed. Certain instructions that \textit{should} trap in R1 don't. If the guest is allowed to execute these, it will see the true state of the CPU, not the virtualized state.
\item A hypervisor typically needs to reserve some portion of the guest's address space for its own use. This is not entirely transparent to the guest OS and may cause clashes.
\item The SYSENTER instruction (used for system calls) executed by an application running in a guest OS always transitions to R0. But that is where the VMM runs, not the guest OS. In this case, the hypervisor must trap and emulate the instruction even when it is not desirable.
\item The CPU segment registers contain a "hidden" descriptor cache which is not software-accessible. The hypervisor cannot read, save or restore this state, but guest OS may use it.
\item Some resources must (and can) be trapped by the hypervisor, but the access is so frequent that this creates a significant performance overhead.
\end{enumerate}
To fix these performance and security issues, VirtualBox contains a \textit{Code Scanning and Analysis Manager} (CSAM), which disassembles guest code, and the \textit{Patch Manager} (PATM), which can replace it at runtime.
\\
\\
Before executing R0 code, CSAM scans it recursively to discover problematic instructions. PATM the performs \textit{in-situ} patching, i.e. it replaces the instruction with a jump to hypervisor memory where an integrated code generator has places a more suitable implementation. In reality, this is a very complex task as there are lots of odd situations to be discovered and handled correctly.
\\
\\
In addition, every time a fault occurs, VirtualBox analyzes the offending code to determine if it is possible to patch it in order to prevent it from causing more faults in the future. This approach works well in practice and dramatically improves software virtualization performance.
\subsection{Hardware virtualization}
As stated in section~\ref{subsec:hw_virt}, with Intel VT-x there are two distinct modes of CPU operation: root mode and non-root mode.
\begin{itemize}
\item In root mode, the CPU operates much like older generations of processors without VT-x support. There are four privilege rings, and the same instruction set is supported, with the addition of several virtualization specific instructions. Root mode is what a host operating system without virtualziation uses, and it is also used by a hypervisor when virtualization is active.
\item In non-root mode, CPU operation is significantly different. There are still four privilege rings and the same instruction set, but a new structure called \textit{Virtual Machine Control Structure} (VMCS) now controls the CPU operation and determines how certain instructions behave. Non-root mode is where guest systems run.
\end{itemize}
The VMCS includes a guest and host state area which is saved/restored when switching between the two modes (\textit{VM entry} and \textit{VM exit}). Most importantly, the VMCS controls which guest operations will cause VM exits.
Thanks to the VMCS, a hypervisor can allow a guest to write certain bits in shadowed control registers,but not others. This enables efficient virtualziation in cases where guests can be allowed to write control bits without disrupting the hypervisor, while preventing them from altering control bits over which the VMM needs to retain full control. The VMCS also provides control over interrupt delivery and exceptions.
\\
\\
Whenever an instruction or event causes a VM exit, the VMCS contains information about the exit reason. Thus the hypervisor can efficiently handle the condition without needing advanced techniques such as CSAM and PATM described above.
\\
\\
VT-x inherently avoids several of the problems which software virtualization faces. The guest has its own completely separate address space not shared with the hypervisor, which eliminates potential clashes. Additionally, guest OS kernel code runs at privilege R0 in non-root mode, obviating the problems by running R0 code at less privileged levels. Naturally, even at R0 in non-root mode, any I/O access by guest code still causes a VM exit, allowing for device emulation.
\\
\\
We restrict our work to the Hardware Virtualization solutions, because the optimizations we introduce are effective in limiting the amount of switches between root mode and non-root mode.
\subsection{Emulation Threads} \label{subsec:EMT}
When a VM is started, a user-defined number of SMP processors is assigned to the VM itself. Each CPU is emulated through a so called \textit{Emulation Thread} (EMT), so we have one EMT per SMP processor. An EMT is responsible of executing guest code, emulating devices and handle the transition between host world and guest world. 
\\
When using hardware virtualization (as we stated before, this is our case), an EMT continuously switches between root mode and VM mode (see section~\ref{subsec:hw_virt}).
\\
\\
Let us assume that the EMT is running guest code, e.g. an application. At some point, the application tries to execute an I/O operation, causing a SYSENTER instruction in the guest. Therefore, the CPU executing the EMT, switches from R3 to R0 (still in VM mode), in order to run the guest kernel code. At this point, the "true" I/O operation (such as a write operation in a register) produces a VM exit, so the CPU switches from VM mode to root mode. On a VM exit the EMT stops executing guest code and start executing VirtualBox code (in kernelspace), in order to handle the event that caused the VM exit itself. Handling a VM exit \textit{may} cause the EMT to execute userspace code in order to emulate a device, switching back to kernelspace after the device emulation (more on this in section~\ref{sec:e1000_emu}). After the event has been handled, the EMT executes a VM entry (i.e. CPU switches back to VM mode) and continues to run guest code from the point where it was interrupted.
\\
\\
\begin{figure}[!ht]
	\centering
	 	\includegraphics[scale=.75]{img/emt_flow.png}
	 	\caption{Example of EMT execution flow}
	 	\label{img:emt_flow}
\end{figure}
Figure~\ref{img:emt_flow} shows the situation described above.

\subsection{Networking modes} \label{subsec:net_modes}
For a VM it is fundamental to communicate to the outside world using the networking infrastructure, otherwise the VM becomes useless.
\\
Nevertheless, a VM is "just" a software entity, so it is not connected to any \textit{real} network. Therefore the VMM must provide some kind of virtualized network infrastructure, so that guest OS thinks its \textbf{virtual} network device is connected to a \textbf{physical} network and can then exchange packets with the outside.
\\
\\
VirtualBox provides several of these network infrastructures called \textit{Networking modes}; in that way a user can choose the most suitable way to connect her VM. Among the others, the main network modes provided by VirtualBox are the following:
\begin{itemize}
\item \textbf{Not attached} In this mode, VirtualBox reports to the guest that a network card is present, but there is no connection, as if no Ethernet cable was plugged into the card.
\item \textbf{Network Address Translation (NAT)} A VM with NAT enabled acts much like a real computer that connects to the Internet through a router. The "router", in this case, is the VirtualBox networking engine, which maps traffic from and to the VM transparently. In VirtualBox this router is placed between each VM and the host.
\\
The VM  receives its network address and configuration on the private network from a DHCP server integrated into VirtualBox. The IP address thus assigned to the VM is usually on a completely different network than the host.

\item \textbf{Bridged networking} With bridged networking, VirtualBox uses a device driver on the host system (\textit{vboxnetflt} kernel module) that filters data from the physical network adapter. That is why it is called "network filter" driver. This allows VirtualBox to intercept data from the physical network and inject data into it, effectively creating a new network interface in software. When a guest is using such a new software interface, it looks to the host system as though the guest were physically connected to the interface using a network cable: the host can send data to the guest through that interface and receive data from it. This means that the user can set up routing or bridging between the guest and the rest of the network.
\item \textbf{Internal networking} It is similar to bridged networking in that the VM can directly communicate with the outside world. However, the "outside world" is limited to other VMs on the same host which connect to the same internal network, which is identified simply by its name.
\\
Even though technically, everything that can be done using internal networking can also be done using bridged networking, there are security advantages with internal networking. In bridged mode, all traffic goes through a physical interface of the host system. It is therefore possible to attach a packet sniffer to the host interface and log all traffic that goes over it. If, for any reason, the user prefers two or more VMs on the same machine to communicate privately, hiding their data from both the host system and the user, bridged networking therefore is not an option.

\item \textbf{Host-only networking} Host-only networking can be thought as a hybrid berween the bridged and internal networking modes: as with bridged networking, the virtual machines can talk to each other and the host as if they were connected through a physical Ethernet switch. Similarly, as with internal networking however, a physical networking interface need not to be present, and the virtual machines cannot talk to the world outside the host since they are not connected to a physical networking interface.
\\
Instead, when host-only networking is used, VirtualBox creates a new software interface on the host (using \textit{vboxnetadp} kernel module) which then appears next to the existing network interfaces. In other words, whereas with bridged networking an existing physical interface is used to attach virtual machines to, with host-only networking a new "loopback" interface is created on the host. And whereas internal networking the traffic between the VMs cannot be seen, the traffic on the "loopback" interface on the host can be intercepted.
\end{itemize}
NAT mode is not interesting with respect to our goals, since it is only intended to be a way the VM can easily access the Internet, and it is not intended to be an efficient networking mode. Similarly, we will not consider bridged networking mode, because optimizing the performance of a real network adapter is not the aim of this work. Instead, we will consider the host-only mode, since our goal is to optimize the communication performances between two VMs on the same host, or between a VM and the host (so also internal networking is not interesting for us), using the netmap framework.

\subsection{Network port and connector} \label{subsec:port_conn}
In order to implement a specific networking architecture, VirtualBox implementation includes an interface between the code that emulates the network adapter, and the code that provides access to the chosen networking mode. The reason is that the two subsystems are completely independent, and a user can easily combine every virtual network adapter with every networking mode.
\\
VirtualBox defines the network device emulation \textit{network port} and the networking mode \textit{network connector}.
\\
\\
Ports and connectors are two interfaces that communicate via a callback mechanism. Whatever implements a port interface, must have a reference (i.e. a pointer) to a connector, and vice versa.
\paragraph{Ports}
The methods~\footnote{The "pfn" prefix stands for "pointer to function". It is imposed by VirtualBox coding guidelines.} exported by ports (which are exposed to connectors) are the following:
\begin{itemize}
\item \texttt{pfnWaitReceiveAvail} Waits until there is space for receiving data. It also takes as argument the number of milliseconds to wait (timeout). If timeout parameter is 0, then it returns immediately. The return value specifies if there is space available to receive data, if timeout expired or if an error occurred.
\item \texttt{pfnReceive} When the connector receives data from the network, it calls this function on the port, so the latter can push data to the guest. This function takes the pointer to available data and the number of bytes in the buffer as arguments.
\item \texttt{pfnReceiveGso} The same as \texttt{pfnReceive}, but it has and additional argument regarding the \textit{segmentation offloading} context~\footnote{We are not going into details, since in our work we did not implement segmentation offloading.}.
\item \texttt{pfnXmitPending} This function is used to notify the port that can transmit pending packets (if any).
\end{itemize}
\paragraph{Connectors}
The methods exported by connectors (which are exposed to ports) are the following:
\begin{itemize}
\item \texttt{pfnBeginXmit} It is used by port to get a lock on the connector (only one port instance can transmit at a time).
\item \texttt{pfnAllocBuf} This asks the connector to provide the buffer that the port will fill with data. The size of that buffer is specified as an argument.
\item \texttt{pfnFreeBuf} Frees an unused buffer that has been requested by the port.
\item \texttt{pfnSendBuf} Sends data to the network. After the port filled the buffer (allocated by the connector) with data, it calls this function passing the filled buffer to the connector.
\end{itemize}

\begin{figure}[!ht]
	\centering
	 	\includegraphics[scale=.75]{img/port_conn_if.png}
	 	\caption{Example of EMT execution flow}
	 	\label{img:port_conn_if}
\end{figure}
In this way, when a port wants to send a frame to the network, it invokes the \texttt{pfnBeginXmit} function provided by the connector in order to gain lock access on the connector itself; then it calls \texttt{pfnAllocBuf} function to get a buffer where the port can store the frame and, finally, it invokes \texttt{pfnSendBuf} so that the connector can push the frame (passed as argument) to the network.
On the other direction, when the connector gets a frame from the network, it invokes the \texttt{pfnWaitReceiveAvail} to whether the port is able to receive data. If this is the case, it immediately calls \texttt{pfnReceive} so that the port can push the received frame (passed as argument) to the guest.
\\
A simplified version of the above interaction is depicted in figure~\ref{img:port_conn_if}, where only the \textit{send} and \textit{receive} functions are shown.

\section{The e1000 network adapter} \label{sec:e1000_adapter}
In this section we will outline of the functioning of an e1000 device. We will describe only those aspects that are relevant to our goals, in particular we are interested to the NIC \textit{datapath}. The complete specifications can be found at~\cite{e1000}
\\
\\
The datapath refers to the software interface that the OS uses in order to transmit and receive Ethernet frames. It involves just some registers and DMA-mapped memory.
\\
When the device driver wants to send an Ethernet frame through the adapter, it has to tell the adapter where the frame is stored in physical memory and how long it is. Once the device is aware of where the frame is stored, it can directly access the physical memory and send the frame on the wire. Similarly, when a frame arrives from the wire, the adapter has to store it in the physical memory. For this reason, it must know in advance where to store the frames, so the device driver must tell the adapter where it can store arrived frames. If it is not the case, the adapter will drop incoming frames.
\\
\\
As we can see, in order to achieve this information exchange, there must be a well-defined interface between the device driver and the adapter. This interface is known as a \textit{ring}. A ring is a circular \textbf{array} (i.e. a countiguous zone in memory) of \textit{descriptors} that are used to exchange  those information. A network device has at least two rings: one for transmission (\textit{TX ring}) and the other for reception (\textit{RX ring}). A network adapter can have multiple TX/RX rings, possibly with different priorities and/or policies, so that it permits traffic engineering.
\\
However, VirtualBox implementation of e1000 device, offers only one ring per direction. The number of descriptors per ring (i.e. the length of the array), can be choosen by the device driver. In e1000 this number must be a power of 2 and less than or equal to 4096.

\subsection{TX ring}
The TX ring is an array containing \textit{N} TX descriptors. Each descriptor is 16 bytes long and contains the physical address of the associated buffer, its length (i.e. the length of the stored frame) and some status flags. Among the others, it contains the \textit{Descriptor Done} flag (DD) and the \textit{End of OPeration} flag (EOP). The DD flag is set by the adapter to tell the device driver that the TX descriptor has been successfully processed. The EOP flag is used by the device driver to tell the adapter whether that TX descriptor contains a complete packet or only a part of it (e.g. because it does not fit in the buffer); so if a packet is spread among \textit{N} TX descriptors, the first \textit{N-1} descriptors will have the EOP flag set to 0, and the last one will have the flag set to 1.
\\
\\
Since the ring is stored in physical memory, the adapter must know its physical address. This information is stored in two registers: TDBAL (\textit{Transmit Descriptor Base Address Low}) and TDBAH (\textit{Transmit Descriptor Base Address High}). These are two 32-bit registers that, concatenated, form a 64-bit string of bits which is the physical base address of the ring.
\\
Since it is a producer/consumer system, a synchronization mechanism is required between the device driver and the adapter. This is achieved using two \textit{index registers}: the TDT register \textit{Transmit Descriptor Tail} and the TDH register \textit{Transmit Descriptor Head}. The value of these registers represent an \textit{array index} with respect to the TX ring.
\\
\\
At the beginning, TDT and TDH are initialized to their initial value (0) by the device driver. When the driver wants to send a new frame, it writes the physical address and the length of the frame in the descriptor pointed by TDT register, then it increments the TDT register itself (modulo number-of-descriptors).
\\
When the adapter recognizes that TDH and TDT are different, it understands that there are new frames to be sent on the wire, so it start processing the descriptors starting from the one pointed by TDH.
\\
For each new descriptor to process, the device:
\begin{enumerate}
\item Sends the new frame on the wire.
\item Writes the TX descriptor back in order to set the DD flag to 1.
\item Increments the TDH register circularly. 
\end{enumerate}

So the adapter can access the next descriptor to be processed with this formula:
$$Index = base + (TDH \times 16)$$
Where \textit{base} is the concatenation of TDBAH and TDBAL registers.
\\
\\
The \textbf{adapter} stop condition is \begin{math}TDH == TDT \end{math}, i.e. there are no more descriptors to process (TX ring empty).
\\
In order to prevent the index registers to wrap around, the device driver must never use the last free descriptor. So when the TDT is such that incrementing it circularly would cause \begin{math}TDH == TDT \end{math}, the driver must stop transmitting. This is the \textit{TX ring full} condition.
\\
Figure~\ref{img:tx_ring} shows the TX ring with its registers.
\begin{figure}[!h] \label{img:tx_ring}
	\centering
	\includegraphics[scale=0.6]{img/tx_ring.png}
	\caption{TX ring with its registers. Free descriptors are the grey ones, while the pending descriptors are the white ones. \textit{base} is the concatenation of TDBAH and TDBAL registers.}
\end{figure}

\subsection{RX ring}
The RX ring is an array of \textit{N} RX descriptors. Each RX descriptor is 16 bytes long and contains the physical address of the associated buffer, its length and some status flags. Among the others, it contains the DD flag and the EOP flag (just like a TX descriptor). The DD flag is used by the adapter to notify the device driver that a the RX descriptor contains new data to be processed. The EOP flag is used by the adapter to tell the device driver whether that RX descriptor contains a complete packet or only a part of it. So if a packet is spread over \textit{N} RX descriptors, the first \textit{N-1} will descriptors will have the EOP flag set to 0, while the last one will have the flag set to 1.
\\
\\
Just as for TX ring, the adapter must know the physical address of the RX ring. This address is stored in two registers: RDBAL (\textit{Receive Descriptor Base Address Low}) and RDBAH (\textit{Receive Descriptor Base Address High}). These are two 32-bit registers that, concatenated, form a 64-bit string of bits which is the physical base address of the RX ring.
\\
Here also, the synchronization between the adapter and the device driver is implemented through two index registers: RDT register (\textit{Receive Descriptor Tail}) and RDH (\textit{Receive Descriptor Head}).
\\
\\
At the beginning, the driver initializes RDH and RDT to their initial values (0). The adapter still does not know the physical address of any buffer where it can store frames, so it cannot store incoming frames. Therefore, the driver writes the physical address of a memory buffer into the RX descriptor pointed by RDT, and increment RDT circularly\footnote{It is useless to write the length field in the descriptor, since it will be modified by the adapter on reception.}. Now the device know that a new memory buffer is available, so it use it to store incoming frames. When $RDH \neq RDT$, the adapter knows that there are buffers available, so it can accept incoming frames.
\\
When a new frame is arrived from the wire, the adapter:
\begin{enumerate}
\item Fetches the RX descriptor pointed by the RDH register.
\item Copies the frame to the buffer pointed by the fetched RX descriptor.
\item Writes back the descriptor in order to set the length of the received frame and the DD bit (\textit{writeback}).
\item Increments RDH register circularly.
\item \textit{May} send an interrupt in order to tell the driver that a new frame is available to be delivered to the network stack\footnote{See section~\ref{subsec:rx_emulation}.}.
\end{enumerate}
So the adapter can access the next available RX descriptor with this formula:
$$Index = base + (RDH \times 16)$$
where \textit{base} is the concatenation of RDBAH and RDBAL registers.
\\
\\
The \textbf{adapter} stop condition is $RDH == RDT$, i.e. there is no available free descriptor where a frame can be stored.
\\
As for the TX ring, in order to prevent the array indexes to wrap around, the driver should never increment RDT if the increment would cause $RDT == RDH$. This is the full RX ring condition.
\\
Figure~\ref{img:rx_ring} shows the RX ring with its registers.
\begin{figure}[!h] \label{img:rx_ring}
	\centering
	\includegraphics[scale=0.6]{img/rx_ring.png}
	\caption{RX ring with its registers. Filled descriptors are the grey ones, while the available descriptors are the white ones. \textit{base} is the concatenation of RDBAH and RDBAL registers.}
\end{figure}

\subsection{e1000 interrupts} \label{subsec:e1000_interrutps}
The e1000 network adapter can generate interrupts for several reasons, but we are only interested in two of them:
\begin{itemize}
\item \textit{TX interrupts}: these interrupts are raised when the transmission of one or more frames completed. Each TX descriptor has a bit flag \textit{Report Status} (RS), that, if set, tells the adapter to raise an interrupt as soon as the associated frame is transmitted. Anyway, an interrupt is always sent when the adapter reaches the stop condition ($TDT == TDH$).
\\
The interrupt handler frees the descriptors that have been processed (DD flag set), and mark them as free (unset the DD flag).
\item \textit{RX interrupts}: these are raised whenever the adapter stores a new incoming frame in physical memory; in that way the device driver knows that a new frame has been received and it can be pushed to the kernel network stack.
\end{itemize}
When we are dealing with high packet rates, e.g. 1 Mpps, interrupt rate becomes a critical issue. In fact, interrupt routines have a fixed cost that must be paid before doing useful work, such as push the received frame to the network stack and let the receiver application to process it.
\\
At this rate, if each received packet raised a RX interrupt, the we would handle up to 1 million of interrupts per second, which would stall the machine. In that case, the CPU would spend almost all of its time in handling interrupts, and the receiver application could not do any useful work. This problem is known as the \textit{livelock} problem.
\\
\\
This problem can be solved if the device "skips" some RX interrupts, raising an interrupt every batch of received frames, e.g. 100 frames per batch, and not every single frame. In that way, the interrupt rate is 100 lower and the interrupt overhead is amortized over 100 frames. Anyway, the device must guarantee that a RX interrupt is raised after a period of time, even if the 100-frames batch is not completed, because the device cannot know when the next frame will arrive.
\\
These mechanisms are known as \textit{interrupt mitigation}. 
\\
The e1000 network adapter implements two interrupt mitigation mechanism, but since the older one is strongly discouraged by the Intel manual, we will consider only the most recent one.
\\
\\
The e1000 network adapter has a register, called \textit{Interrupt Throttling Register} (ITR), which controls the interrupt mitigation mechanism. If the driver sets this register to a value of $\delta$, the hardware ensures that $\delta$ is the minimum inter-interrupt interval, regardless of the interrupt type. In other words, whenever an event that requires an interrupt occurs, such as TX completion or RX completion, the device raises an interrupt as soon as possible, while meeting the ITR inter-interrupt delay constant.
\section{VirtualBox e1000 emulation}  \label{sec:e1000_emu}
The e1000 port is implemented in VirtualBox through three source files \footnote{These source files are located in src/VBox/Devices/Network/ in the VirtualBox project root directory.}: \textit{DevE1000.cpp}, which is the one we are interested in, \textit{DevE1000Phy.cpp} and \textit{DevEEPROM.cpp}. The first one contains all the "emulation logic" part, while the others implement only the internal physical emulation and the internal EEPROM respectively.
\\
The code is an implementation of the interfaces provided by VirtualBox. Besides the callbacks discussed before (used for communication between port and connectors) there are other callbacks that provide an interface between ports and the rest of VirtualBox emulator: among the others, we have a  \textit{constructor} and a \textit{destructor}. The purpose of these functions is to initialize/uninitialize variables and structures used in the emulation, and to register/deregister the new PCI Ethernet device with the rest of the emulator. In this way, it is possible to have multiple instances of the e1000 network device when launching VirtualBox.
\\
Furthermore, the code contains (in the first part of the file), a set of \textit{options} , implemented as \texttt{define} statements, that can be enabled/disabled before compiling (e.g. enable usage of caches, ITR register, and so on).
\\
\\
When registering a new PCI device, it is necessary to describe the I/O or MMIO regions that the device implements the device: this is done registering callback functions to to those regions, one for \textit{in} operations, on for \textit{out} operations.
\\
The e1000 emulation code registers a MMIO region and an I/O region, but the latter is not used. The MMIO region maps all the register the e1000 device implements.
\\
A statically defined mapping table is used to associate a couple of functions to each register, one for \textit{IN} operation, one for \textit{OUT} operation. In that way, one may associate a different read callback and a different write callback for each e1000 register. Is also possible to have the same callback function for multiple registers, or have no callbacks for some of them.
\\
In short, the emulation of a device is achieved with pre-registered callbacks.
\\
\\
Now we will see in details how the register callbacks are invoked.
\\
When an EMT is executing guest code, e.g. the e1000 device driver, it may try to access a MMIO location corresponding to an e1000 register. The accessing instruction causes a VM exit, so the EMT switches from the guest context to the host context. At this point, the VirtualBox driver analyzes the VM exit reason and understand that the VM exit was caused by an MMIO access. In our case, the callback registered with the e1000 MMIO is invoked. This callback uses the address to get the index of the accessed register and calls the read (write) handler specific for that register.
\\
After the callback returns, a VM entry is executed and the EMT resumes executing guest code.
\subsubsection{Event queues}
A register callback is invoked by the EMT (section~\ref{subsec:EMT}) while it is handling the VM exit event. So we are in R0 context. That said, writing a register may cause some side effects, therefore these side effects should be emulated. However, since it may take an amount of time that is, possibly, much longer than a simple register update, and also because it is unnecessary to execute that code in kernelspace, it is reasonable to execute the "side-effect" code in userspace.
\\
In order to achieve this, VirtualBox provides the so called \textit{Event queues}. As the name suggests, these are queues in which an \textit{event} can be posted. Each queue has a callback function that is associated to the queue itself.
\\
Every time the EMT finishes handling a VM exit it checks all the event queues before executing a VM entry. If an event is found, the EMT removes that event from the queue and switches from R0 context to R3 context, so that it can start executing the callback associated to that queue. 
\\
When the callback returns, the EMT switches back to the kernelspace and continues scanning all event queues. Finally, when there are no more pending events, it executes a VM entry and resumes executing guest code.

\subsection{TX emulation}
The TX execution path is performed in by the EMT thread. As described in section ~\ref{sec:e1000_adapter}, when the device driver wants to notify the hardware that a new TX frame is ready to be processed, it writes to the TDT register. This causes a VM exit to occur, so the EMT passes from guest context to host context.
\\
Writing to the TDT register causes the adapter to transmit one or more frames, so the transmission handling must be done in userspace. So the write callback for the TDT register, aside from updating the register value, posts an event to an event queue (\textit{TxQueue}); in this way the EMT, before switching back to the guest world, executes the transmission of frames in userspace.
\\
The callback associated with the event queue simply calls the \texttt{e1kXmitPending} function. The way this function is implemented is regulated from the \texttt{E1K\_WITH\_TXD\_CACHE} define.
\\
\\ 
When TX descriptors cache is disabled, the \texttt{e1kXmitPending} function:
\begin{enumerate}
\item \textit{Tries to acquire the lock} It calls the \texttt{pfnBeginXmit} function provided by the connector, where it \textbf{tries} to acquire the lock on the connector itself. This is a so called \textit{trylock} function, because if the lock is busy, the thread does not block waiting for the lock to be acquired. Instead, the function returns a \textit{busy} value. This is necessary because the thread trying to get the lock is the EMT, so it \textbf{must never} block, since it is in charge of emulating all the system.
If the trylock succeeds, then go to the next step, otherwise the function returns without doing any transmission.
\item \textit{Loads the current descriptor} If there are available descriptors ($TDT \neq TDH$), then it reads the TX descriptor pointed by TDH register from the guest physical memory.
\item \textit{Allocates buffer} The port asks the \textit{connector} to allocate the buffer where it can store the frame, which is pointed by the loaded TX descriptor. The information about the length of the frame is contained in the TX descriptor itself.
\item \textit{Writes back the descriptor} It writes the descriptor back into the guest physical memory, setting the DD bit to 1.
\item \textit{Copies the frame} It reads the physical address pointed by the current descriptor (where the frame is stored) and copies it into the buffer provided by the connector.
\item \textit{Possibly Transmits frame} If the packet is complete, i.e. EOP flag of TX descriptor is set to 1, then it calls the \texttt{pfnSendBuf} provided by the connector, passing the previously allocated buffer as an argument.
\item \textit{Updates register} It circularly increments the TDH register.
\item \textit{Possibly raises an interrupt} If at least one complete packet has been sent, it \textit{may} raise an interrupt to notify the guest that one or more frames have been sent (see section~\ref{subsec:e1000_interrutps}). If there are other descriptors available ($TDT \neq TDH$), then go back to step 2.
\item \textit{Releases the lock} After all operations, it releases the lock acquired at step 1 calling the \texttt{pfnEndXmit} provided by the connector.
\end{enumerate}
We can see that for each available descriptor, the EMT must read the guest physical memory in order to get the current TX descriptor. This introduces an high overhead, particularly when the number of available descriptors is much higher than 1.
\\
\\
The reading overhead can be amortized using a local TX descriptor cache. This can be done enabling the related option (\texttt{define E1K\_WITH\_TXD\_CACHE}).
\\
When the cache is enabled, the \texttt{e1kXmitPending} function behaves much like the same as before, except for the step 2. Previously, we had one TX descriptor per read. Now, instead, the EMT \textbf{tries} to load all the available TX descriptors in one single physical read (two reads in case the tail wrapped around the end of the TX descriptor ring). However, it may happen that only a fraction of the available descriptors is loaded, e.g. the cache is almost full. In that case, after the partial loading, the fetched descriptors are processed (like the previous case), the frames are sent and the cache is flushed, so that other available descriptors can be loaded.
\\
In that way, the cost of the readings is amortized over the number of fetched descriptors.

\subsection{RX emulation} \label{subsec:rx_emulation}
This time the RX execution path is not performed by the EMT, but it is performed by another thread. The reason is that the EMT is in charge of the entire emulation process, so it must never do blocking operations, so it cannot wait for incoming frames from the network.
\\
The thread in charge of receive frames is dependent on the used connector: in fact the frames flow from the network to the connector, and the latter sends them to the port (see section~\ref{subsec:port_conn}, figure~\ref{img:port_conn_if}).
\\
From now on, we will refer to this thread as \textit{recv thread}.
\\
\\
When one or more frames from the network, the recv thread stores them in a buffer (that is dependent from the connector implementation). Then it first calls the port callback \texttt{pfnWaitReceiveAvail} to check if the port is able to receive at least one frame. If that callback returns successfully, the thread calls the \texttt{pfnReceive} callback, passing the buffer with the available data.
\\
\\
The e1000 port implements the \texttt{pfnReceive} method with \texttt{e1kR3NetworkDown\_Receive} function, which takes the buffer and the size of the available data in the buffer as arguments.
\\
As in the TX case, the behavior (i.e. the implementation) is controlled by the \texttt{E1K\_WITH\_RXD\_CACHE} define.
\\
\\
When RX descriptor cache is disabled, this function:
\begin{enumerate}
\item \textit{Filters the packet} It determines if the packet is to be delivered to the upper layer. The decision is based on:
	\begin{itemize}
	\item Length: if the packet length is greater than the maximum supported size (16384 bytes) or if long~\footnote{Packet greater than 1522 bytes.} packet reception is disabled~\footnote{For further information, see~\cite{e1000}.}, then drop the packet.
	\item VLAN tag: if the filter does not find a match for the VLAN tag, then drop the packet.
	\item Exact Unicast/Multicast: if the packet destination address exactly matches the address (either unicast or multicast address), then deliver the packet, otherwise drop it.
	\item Promiscuous Unicast/Multicast: if the adapter is set in promiscuous mode~\footnote{The adapter accepts packets even if they are not addressed to it}, then deliver the packet, otherwise drop it.
	\item Broadcast: if the packet destination address is broadcast, deliver it.
	\end{itemize}
\item \textit{Pads the packet} If the received packet is too short (less than 60 bytes), the packet is padded with zeroes.
\item \textit{Loads the RX descriptor} If there is at least one available RX descriptor ($RDT \neq RDH$), then it loads the RX descriptor pointed by RDH register from the guest physical memory.
\item \textit{Fills the buffer} After loading the RX descriptor, it writes the (possibly padded) packet into the buffer pointed by the loaded RX descriptor. If the packet does not fit the buffer, the EOP flag of the RX descriptor is set to 0, otherwise the flag is set to 1.
\item \textit{Writes back the RX descriptor} After the RX descriptor has been filled with data, it sets the DD bit to 1 and writes back the RX descriptor in memory.
\item \textit{Updates register} It circularly increments the RDH register.
\item \textit{Possibly raises an interrupt} If at least one complete packet is successfully delivered, it \textit{may} raise an interrupt to notify the guest that one or more frames have been received (see section~\ref{subsec:e1000_interrutps}). If there is still data to be processed (e.g. a packet is not completely stored), go back to step 3.
\end{enumerate}
When the reception is completed (i.e. the \texttt{pfnReceive} function returns), the recv thread goes back to sleep waiting for new incoming packets.
\\
\\
Just as in the TX case, we can see that for each available descriptor, the recv thread must read the guest physical memory to get the current RX descriptor. Therefore we have an high overhead, in particular when we are dealing with large incoming packets.
\\
\\
This overhead can be amortized using a local RX descriptor cache, that can be enabled through the related option (\texttt{define E1K\_WITH\_RXD\_CACHE}). When the cache is enabled, the \texttt{e1kR3NetworkDown\_Receive} function behaves much like before, except for what concerns the loading of RX descriptors. In the previous case, we had one single RX descriptor per read. Now the recv thread, instead of loading the descriptor in memory, looks at the local cache: if the cache is empty, it \textit{prefetches} a number of RX descriptors, which is the minimum between the cache size and the number of available RX descriptors in memory, with a single memory read (two reads in case the tail wrapped around the end of the RX ring). If the cache is not empty, it uses the first available RX descriptor in cache.
\\
After the RX descriptor is processed and written back, the related position in cache is cleaned.
\\
\\
It is important to point out that the physical \textbf{guest} memory is different from the physical \textbf{host} memory. Therefore, all the accesses to the guest physical memory (such as TX/RX descriptor loads) require an \textit{address translation}, that introduces additional overhead. The way it can be obviated will be described in section~\ref{subsec:memmap}.

\chapter{Implementation of netmap support}
In chapter 2 we described the VirtualBox architecture, its implementation and the interfaces it provides. In this chapter we will implement an extension to VirtualBox that provides fast packet I/O. This is achieved by interfacing VirtualBox with the netmap framework~\cite{netmap}.
\\
\\
As described in section~\ref{subsec:port_conn}, VirtualBox includes two interfaces in order to implement the networking infrastructure: \textit{ports} and \textit{connectors}. Our goal is to create a new connector that implements the interfaces a VirtualBox port to a VALE switch~\cite{vale} provided by the netmap framework.
\\
Our netmap connector will be implemented through one single source file: \textit{DrvNetmap.cpp}\footnote{This source file will be located in src/VBox/Devices/Network/ in the VirtualBox project root directory. The \textit{Drv} prefix in the filename is due to the VirtualBox coding guidelines.}.

\section{Integration with VirtualBox}
Before going into details of our implementation, we must first "integrate" our work in VirtualBox system. It basically involves the VirtualBox build process, the user interface, so that a user can choose netmap as connector for his system, and the VMs configuration mechanism, in order to set/save/load the new configuration parameters needed by netmap.

\subsection{Building VirtualBox}
The VirtualBox build process is performed through two steps: the configuration and the compilation.

\subsubsection{Configuration}
Before starting the compilation process, we must run the script \textit{configure}\footnote{Located in the root directory of VirtualBox project.}, in order to check if the system meets the requirements, e.g. the presence of needed libraries, and to let the user to specify some custom options (e.g. the \texttt{--disable-docs} option prevents the compilation of documentation files). Therefore, we added the "\texttt{--with-netmap=}\textit{dir}" option to that script, so that the user can specify the absolute path~\footnote{It is requested the path to the subdirectory \textit{sys} of netmap source tree, e.g. \texttt{--with-netmap=/path/to/netmap/sys/}.} of netmap libraries in the system.
\\
The effect of the new option is to set an environment variable that the Makefile will check, so that it knows whether to include or not our \textit{DrvNetmap.cpp} source file in the compilation process, and the netmap headers in the list of directories to be searched for header files\footnote{\texttt{-I} option of GNU C compiler.}.

\subsubsection{Compilation}
As mentioned before, the Makefile will check a netmap-related environment variable that can be enabled through the configuration process. VirtualBox has one \textit{virtual} Makefile, that is actually splitted in many files (more than two hundreds). Those files are organized in a hierarchical manner: there is one Makefile for each level of the source tree. Therefore, the more we go deeper in the source tree, the more specific the present Makefile is for that subtree.
\\
Since our new file will be placed in the network devices directory, we will modify the devices-related Makefile, located in \textit{src/VBox/Devices/} directory.

\subsection{The VirtualBox User Interface}
As stated in section~\ref{sec:features}, VirtualBox offers multiple frontends as user interfaces. We need to extend those interfaces so that a user can choose netmap as connector.
\\
We focused on the \textit{VirtualBox} frontend (the default one), and \textit{VBoxManage} frontend, since \textit{VBoxSDL} only launches a VM, but the related configuration is done through VBoxManage.

\subsubsection{VirtualBox frontend}
This is the default frontend. It offers a user-friendly graphical user interface (GUI).
\section{Initialization}
\section{Send side}
\section{Receive side}
\chapter{Optimizations on e1000 emulated device}
\section{Analysis of current implementation}
\subsection{TX performance}
\subsection{RX performance}
\section{Implementing mapping of descriptors} \label{subsec:memmap}
\subsection{Implementation}
\subsection{Performance analysis}
\section{Packet batching}
\subsection{Implementation}
\subsection{Modification of netmap implementation}
\subsection{Performance analysis}
\chapter{Conclusions}

%\addcontentsline{toc}{section}{\refname}
\begin{thebibliography}{30}
\bibitem{vbox} VirtualBox project. \url{https://www.virtualbox.org}
\bibitem{netmap} \autori{Rizzo L.} \textit{netmap: a novel framework for fast packet I/O} \url{http://info.iet.unipi.it/~luigi/papers/20120503-netmap-atc12.pdf}
\bibitem{vale} \autori{Rizzo L., Lettieri G.} \textit{VALE: a switched ethernet for virtual machines.} \url{http://info.iet.unipi.it/~luigi/papers/20121026-vale.pdf}
\bibitem{amd-v} \autori{AMD} \textit{Secure Virtual Machine Architecture Reference Manual.}
\bibitem{vt-x} \autori{Neiger, Gil Santoni, A.} Intel virtualization technology: Hardware support for efficient processor virtualization. \textit{Intel Technology Journal 10, 3 (2006)}
\bibitem{iommu} \autori{Yehuda, B.} Utilizing IOMMUs for virtualization in linux and xen.
\bibitem{vmm_opt} \autori{Agensen, O., Mattson, J., Rugina, R., Sheldon, J.} Software techniques for avoiding hardware virtualization exits.
\bibitem{virtio} \autori{Russel, R.} virtio: towards a de-facto standard for virtual I/O devices.
\bibitem{rings} \autori{Intel} \textit{Intel 64 and IA-32 Architectures Software Developer's Manual}.
\bibitem{e1000} \autori{Intel} \textit{PCI/PCI-X Family of Gigabit Ethernet Controllers Software Developers Manual.}
\bibitem{sdl} SDL project. \url{http://www.libsdl.org/index.php}
\bibitem{qt} Qt project. \url{http://www.qt.io/}
\end{thebibliography}
	
\end{document}
